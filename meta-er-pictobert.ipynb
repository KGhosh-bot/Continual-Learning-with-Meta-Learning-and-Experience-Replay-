{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Continual Learning for personalized pictogram recommendation using PictoBERT\n","\n","A pictogram is a picture with a label that denotes an action, object, person, animal, or place. Predicting the next pictogram to be set in a sentence in construction is an essential feature for AAC boards to facilitate communication.\n","\n","PictoBERT, an adaptation of BERT for the next pictogram prediction task, with changed input embeddings to allow word-sense usage instead of words, considering that a word-sense represents a pictogram.\n","\n","Continual learning (CL) aims to adaptively learn across time by leveraging previously learned data to improve generalization for future data."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:48:37.276870Z","iopub.status.busy":"2024-08-29T18:48:37.276544Z","iopub.status.idle":"2024-08-29T18:48:51.844271Z","shell.execute_reply":"2024-08-29T18:48:51.843021Z","shell.execute_reply.started":"2024-08-29T18:48:37.276836Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting gdown\n","  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.3)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.15.1)\n","Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.7.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n","Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n","Installing collected packages: gdown\n","Successfully installed gdown-5.2.0\n"]}],"source":["!pip install gdown"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:48:52.957471Z","iopub.status.busy":"2024-08-29T18:48:52.957067Z","iopub.status.idle":"2024-08-29T18:48:56.595518Z","shell.execute_reply":"2024-08-29T18:48:56.594713Z","shell.execute_reply.started":"2024-08-29T18:48:52.957431Z"},"trusted":true},"outputs":[],"source":["# For data downloads\n","import gdown\n","import pickle\n","import json\n","import pandas as pd\n","\n","# For ARASAAC API\n","import requests\n","\n","# For model\n","import torch\n","# from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig\n","\n","# For text processing\n","import string\n","\n","# For dataset creation\n","# from datasets import Dataset\n","import random\n","\n","import gc\n","from IPython.display import Markdown, display\n","\n","def display_markdown(markdown_text: str):\n","    display(Markdown(markdown_text))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data SemChilds\n","\n","The data is the automatically annotated the North American English part of the Child Language Data Exchange System (CHILDES) corpus (MacWhinney, 2014) with word-senses to used as a training corpus for PictoBert, called the SemCHILDES."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:48:56.597467Z","iopub.status.busy":"2024-08-29T18:48:56.596995Z","iopub.status.idle":"2024-08-29T18:49:19.412709Z","shell.execute_reply":"2024-08-29T18:49:19.411642Z","shell.execute_reply.started":"2024-08-29T18:48:56.597433Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading...\n","From (original): https://drive.google.com/uc?id=1cGD5317jxdVFLk35KGkaTErYnSyGrJDr\n","From (redirected): https://drive.google.com/uc?id=1cGD5317jxdVFLk35KGkaTErYnSyGrJDr&confirm=t&uuid=b6eef3c1-7250-476d-9728-c234981b97ac\n","To: /kaggle/working/train_data.pt\n","100%|██████████| 247M/247M [00:01<00:00, 162MB/s] \n","Downloading...\n","From: https://drive.google.com/uc?id=1a4crU1Vq6ujRXmcceVeK0qDXu-H31Ml2\n","To: /kaggle/working/eval_data.pt\n","100%|██████████| 2.52M/2.52M [00:00<00:00, 56.3MB/s]\n","Downloading...\n","From: https://drive.google.com/uc?id=1qM4ZeSs51QO85CAlSBgjqYAnyL8ySq6d\n","To: /kaggle/working/test_data.pt\n","100%|██████████| 2.52M/2.52M [00:00<00:00, 158MB/s]\n"]}],"source":["train_data_url = 'https://drive.google.com/uc?id=1cGD5317jxdVFLk35KGkaTErYnSyGrJDr'\n","train_data_output = 'train_data.pt'\n","gdown.download(train_data_url, train_data_output, quiet=False)\n","with open(train_data_output, 'rb') as file:\n","    # Load the object from the file\n","    train_data = pickle.load(file)\n","\n","eval_data_url = 'https://drive.google.com/uc?id=1a4crU1Vq6ujRXmcceVeK0qDXu-H31Ml2'\n","eval_data_output = 'eval_data.pt'\n","gdown.download(eval_data_url, eval_data_output, quiet=False)\n","with open(eval_data_output, 'rb') as file:\n","    # Load the object from the file\n","    eval_data = pickle.load(file)\n","\n","data_url = 'https://drive.google.com/uc?id=1qM4ZeSs51QO85CAlSBgjqYAnyL8ySq6d'\n","data_output = 'test_data.pt'\n","gdown.download(data_url, data_output, quiet=False)\n","with open(data_output, 'rb') as file:\n","    # Load the object from the file\n","    test_data = pickle.load(file)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T14:30:59.141032Z","iopub.status.busy":"2024-08-25T14:30:59.140673Z","iopub.status.idle":"2024-08-25T14:30:59.148412Z","shell.execute_reply":"2024-08-25T14:30:59.147463Z","shell.execute_reply.started":"2024-08-25T14:30:59.141000Z"},"trusted":true},"outputs":[{"data":{"text/plain":["dict_keys(['input_ids', 'attention_mask', 'special_tokens_mask', 'ngrams'])"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["keys = train_data.keys()\n","keys"]},{"cell_type":"markdown","metadata":{},"source":["### Data Visualization"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T14:31:00.391321Z","iopub.status.busy":"2024-08-25T14:31:00.390951Z","iopub.status.idle":"2024-08-25T14:31:00.396800Z","shell.execute_reply":"2024-08-25T14:31:00.395822Z","shell.execute_reply.started":"2024-08-25T14:31:00.391284Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["input_ids:  [13580, 190, 7, 11, 6, 1302, 205, 1, 13579, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581]\n","attention_mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","special_tokens_mask:  [1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"]}],"source":["for j in ['input_ids', 'attention_mask', 'special_tokens_mask']:\n","    for i in train_data[j][:1]:\n","        print(f\"{j}: \", i)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T14:31:01.716390Z","iopub.status.busy":"2024-08-25T14:31:01.716016Z","iopub.status.idle":"2024-08-25T14:31:01.722394Z","shell.execute_reply":"2024-08-25T14:31:01.721454Z","shell.execute_reply.started":"2024-08-25T14:31:01.716344Z"},"trusted":true},"outputs":[{"data":{"text/plain":["936379"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["len(train_data['input_ids'])"]},{"cell_type":"markdown","metadata":{},"source":["### Using half the data for training to reduce the training time and due to memory bounds"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:49:22.658382Z","iopub.status.busy":"2024-08-29T18:49:22.657971Z","iopub.status.idle":"2024-08-29T18:49:22.684120Z","shell.execute_reply":"2024-08-29T18:49:22.683096Z","shell.execute_reply.started":"2024-08-29T18:49:22.658343Z"},"trusted":true},"outputs":[{"data":{"text/plain":["468189"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["set_size = len(train_data['input_ids']) // 2\n","\n","# Reduce dataset size by half\n","reduced_dataset = {\n","    \"input_ids\": train_data[\"input_ids\"][:set_size],\n","    \"attention_mask\": train_data[\"attention_mask\"][:set_size],\n","    \"special_tokens_mask\": train_data[\"special_tokens_mask\"][:set_size]\n","}\n","\n","len(reduced_dataset['input_ids'])"]},{"cell_type":"markdown","metadata":{},"source":["### Specifing the parameters"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:49:23.645259Z","iopub.status.busy":"2024-08-29T18:49:23.644626Z","iopub.status.idle":"2024-08-29T18:49:23.650518Z","shell.execute_reply":"2024-08-29T18:49:23.649497Z","shell.execute_reply.started":"2024-08-29T18:49:23.645217Z"},"trusted":true},"outputs":[],"source":["MAX_EPOCHS = 3\n","WARMUP_STEPS = int(MAX_EPOCHS * 0.15)\n","BATCH_SIZE = 32\n","NUM_WORKERS = 2\n","GPUS = 1\n","LEARNING_RATE = 1e-06\n","ACCUMULATE_GRAD_BATCHES = 4\n","LOGGER_VERSION = '1e06'\n","LOGGER_INFO = \"first_run\"\n","FREEZE_TO = None\n","MLM_PROBABILITY= 0.15"]},{"cell_type":"markdown","metadata":{},"source":["### CHILDES Tokenizer"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:49:27.550307Z","iopub.status.busy":"2024-08-29T18:49:27.549575Z","iopub.status.idle":"2024-08-29T18:49:27.572704Z","shell.execute_reply":"2024-08-29T18:49:27.571989Z","shell.execute_reply.started":"2024-08-29T18:49:27.550268Z"},"trusted":true},"outputs":[],"source":["from transformers import PreTrainedTokenizerFast\n","TOKENIZER_PATH = \"/kaggle/input/tokenizer/childes_all_new.json\"\n","f = open(TOKENIZER_PATH)\n","semchilds_tokenizer = json.load(f)\n","#TOKENIZER_PATH = \"/content/drive/MyDrive/TESI/DOCS/tokenizer_arasaac.json\"\n","#TOKENIZER_PATH = \"tokenizer_arasaac.json\"\n","loaded_tokenizer = PreTrainedTokenizerFast(tokenizer_file=TOKENIZER_PATH)\n","loaded_tokenizer.pad_token = \"[PAD]\"\n","loaded_tokenizer.sep_token = \"[SEP]\"\n","loaded_tokenizer.mask_token = \"[MASK]\"\n","loaded_tokenizer.cls_token = \"[CLS]\"\n","loaded_tokenizer.unk_token = \"[UNK]\""]},{"cell_type":"markdown","metadata":{},"source":["Using the CHILDES dictionary to create a mapping to decode the tokenized sentences."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T12:26:48.276612Z","iopub.status.busy":"2024-08-28T12:26:48.275921Z","iopub.status.idle":"2024-08-28T12:26:48.285360Z","shell.execute_reply":"2024-08-28T12:26:48.284470Z","shell.execute_reply.started":"2024-08-28T12:26:48.276565Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{0: '[UNK]', 13579: '[SEP]', 13580: '[CLS]', 13581: '[PAD]', 13582: '[MASK]'}"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["word_sense_to_token_dict = semchilds_tokenizer[\"model\"][\"vocab\"]\n","\n","token_to_word_sense_dict = {v: k for k, v in word_sense_to_token_dict.items()}\n","\n","special_tokens_dict = {i[\"id\"]:i[\"content\"] for i in semchilds_tokenizer[\"added_tokens\"]}\n","special_tokens_dict"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T14:33:42.435479Z","iopub.status.busy":"2024-08-25T14:33:42.434976Z","iopub.status.idle":"2024-08-25T14:33:42.441983Z","shell.execute_reply":"2024-08-25T14:33:42.441119Z","shell.execute_reply.started":"2024-08-25T14:33:42.435445Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[13580, 49, 256, 1277, 8, 834, 110, 1, 13579, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581]\n","[13580, 6, 149, 484, 76, 327, 1, 13579, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581]\n","[13580, 5, 8, 26, 639, 7, 23, 132, 157, 1, 13579, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581, 13581]\n","[CLS]\n","okay%5:00:00:satisfactory:00\n","next%5:00:00:succeeding:00\n","day%1:28:01::\n","be%2:42:06::\n","sunday%1:28:00::\n","again%4:02:00::\n",".\n","[SEP]\n","\n","[CLS]\n","a\n","horse%1:05:00::\n","inside%4:02:00::\n","or\n","outside%4:02:00::\n",".\n","[SEP]\n","\n","[CLS]\n","i\n","be%2:42:06::\n","go_to%2:42:00::\n","try%2:37:00::\n","it\n","in%4:02:01::\n","chair%1:06:00::\n","dad%1:18:00::\n",".\n","[SEP]\n","\n"]}],"source":["sentences = []\n","for i in train_data[\"input_ids\"][2:5]:\n","  print(i)\n","  sentences.append(i)\n","\n","for i in sentences:\n","  for j in i:\n","    if j in token_to_word_sense_dict.keys():\n","      print(token_to_word_sense_dict[j])\n","    elif j in special_tokens_dict.keys() and j != 13581:\n","      print(special_tokens_dict[j])\n","  print(\"\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T14:33:42.813961Z","iopub.status.busy":"2024-08-25T14:33:42.813642Z","iopub.status.idle":"2024-08-25T14:33:42.819599Z","shell.execute_reply":"2024-08-25T14:33:42.818624Z","shell.execute_reply.started":"2024-08-25T14:33:42.813928Z"},"trusted":true},"outputs":[{"data":{"text/plain":["13582"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["loaded_tokenizer.mask_token_id"]},{"cell_type":"markdown","metadata":{},"source":["### Loading the data and defining the dataloader"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:49:31.208936Z","iopub.status.busy":"2024-08-29T18:49:31.208221Z","iopub.status.idle":"2024-08-29T18:49:31.769891Z","shell.execute_reply":"2024-08-29T18:49:31.768910Z","shell.execute_reply.started":"2024-08-29T18:49:31.208895Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset, Subset\n","from torch import tensor\n","from torch.nn.utils.rnn import pad_sequence\n","from sklearn.model_selection import train_test_split\n","import pickle\n","\n","class MyDataset(Dataset):\n","    def __init__(self, examples):\n","\n","        self.input_ids = examples['input_ids']\n","        self.attention_mask = examples['attention_mask']\n","        self.special_tokens_mask = examples['special_tokens_mask']\n","        self.labels = None\n","        if 'labels' in examples:\n","            self.labels = examples['labels']\n","        self.pad_token_id = loaded_tokenizer.pad_token_id\n","        self.max_len = 32\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def pad(self, sequence, pad_length, pad_value):\n","        \"\"\"\n","        Pads a sequence with a specific value to a desired length.\n","\n","        Args:\n","          sequence: The sequence to be padded.\n","          pad_length: The desired length after padding.\n","          pad_value: The value to use for padding.\n","\n","        Returns:\n","          The padded sequence.\n","        \"\"\"\n","        return sequence + [pad_value] * pad_length\n","\n","\n","    def __getitem__(self, idx):\n","        input_ids = tensor(self.input_ids[idx])\n","        attention_mask = tensor(self.attention_mask[idx])\n","        special_tokens_mask = tensor(self.special_tokens_mask[idx])\n","\n","        out_dict = {\n","          \"input_ids\":input_ids,\n","          \"attention_mask\":attention_mask,\n","          \"special_tokens_mask\":special_tokens_mask\n","        }\n","\n","        if self.labels is not None:\n","            out_dict['labels'] = self.labels[idx]\n","\n","        return out_dict\n","\n","\n","train_dataset = MyDataset(reduced_dataset)\n","\n","val_dataset = MyDataset(eval_data)\n","\n","test_dataset = MyDataset(test_data)\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:49:31.771931Z","iopub.status.busy":"2024-08-29T18:49:31.771466Z","iopub.status.idle":"2024-08-29T18:49:43.928588Z","shell.execute_reply":"2024-08-29T18:49:43.927766Z","shell.execute_reply.started":"2024-08-29T18:49:31.771899Z"},"trusted":true},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","data_collator = DataCollatorForLanguageModeling(tokenizer=loaded_tokenizer, mlm_probability=MLM_PROBABILITY)\n","\n","def data_collator_new(examples):\n","\n","    batch = {\n","      \"input_ids\" : torch.stack([example['input_ids'] for example in examples]),\n","      \"attention_mask\": torch.stack([example['attention_mask'] for example in examples]),\n","  }\n","  # Clone input_ids to labels\n","    labels = batch[\"input_ids\"].clone()\n","  # Initially set all labels to -100 to ignore them in the loss computation\n","    labels.fill_(-100)\n","\n","  # Iterate over each sequence in the batch\n","    for idx, sequence in enumerate(batch[\"input_ids\"]):\n","      # Find the last occurrence of the specific token\n","        token_positions = (sequence == token_id).nonzero(as_tuple=True)[0]\n","        if len(token_positions) > 0:\n","            last_token_position = token_positions[-1]\n","          # Set labels for tokens after the specific token to their actual values\n","            if last_token_position + 1 < sequence.size(0):  # Check if there is a next token\n","                labels[idx, last_token_position + 1:] = batch[\"input_ids\"][idx, last_token_position + 1:]\n","\n","  # Update the inputs dictionary to include the adjusted labels\n","    batch['labels'] = labels\n","\n","    return batch"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:49:43.930656Z","iopub.status.busy":"2024-08-29T18:49:43.930072Z","iopub.status.idle":"2024-08-29T18:49:43.937487Z","shell.execute_reply":"2024-08-29T18:49:43.936600Z","shell.execute_reply.started":"2024-08-29T18:49:43.930621Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torch.utils.data.distributed import DistributedSampler\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=BATCH_SIZE,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=True,\n","    collate_fn=data_collator,\n","    drop_last = True,\n","    shuffle=True\n",")\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=BATCH_SIZE,\n","    num_workers=NUM_WORKERS,\n","    pin_memory=True,\n","    collate_fn=data_collator,\n","    drop_last = True\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE,\n","    num_workers=NUM_WORKERS,\n","    collate_fn=data_collator,\n","    pin_memory=True,\n","    drop_last = True\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:49:46.198992Z","iopub.status.busy":"2024-08-29T18:49:46.198596Z","iopub.status.idle":"2024-08-29T18:49:46.789649Z","shell.execute_reply":"2024-08-29T18:49:46.788592Z","shell.execute_reply.started":"2024-08-29T18:49:46.198953Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Input ids:  tensor([[13580,    15,    22,  ..., 13581, 13581, 13581],\n","        [13580,     5, 13582,  ..., 13581, 13581, 13581],\n","        [13580, 13582,    19,  ..., 13581, 13581, 13581],\n","        ...,\n","        [13580,    40,    72,  ..., 13581, 13581, 13581],\n","        [13580,     5,    21,  ..., 13581, 13581, 13581],\n","        [13580,    17,    19,  ..., 13581, 13581, 13581]]) torch.Size([32, 32])\n","labels:  tensor([[-100, -100, -100,  ..., -100, -100, -100],\n","        [-100, -100,  442,  ..., -100, -100, -100],\n","        [-100,    5, -100,  ..., -100, -100, -100],\n","        ...,\n","        [-100, -100, -100,  ..., -100, -100, -100],\n","        [-100, -100, -100,  ..., -100, -100, -100],\n","        [-100, -100, -100,  ..., -100, -100, -100]]) torch.Size([32, 32])\n"]}],"source":["for batch in train_dataloader:\n","    print(\"Input ids: \", batch['input_ids'], batch['input_ids'].shape)\n","    print(\"labels: \", batch['labels'], batch['labels'].shape)\n","    break"]},{"cell_type":"markdown","metadata":{},"source":["## Loading the Pretrained Model PictoBERT"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:49:49.358290Z","iopub.status.busy":"2024-08-29T18:49:49.357856Z","iopub.status.idle":"2024-08-29T18:49:51.921426Z","shell.execute_reply":"2024-08-29T18:49:51.920488Z","shell.execute_reply.started":"2024-08-29T18:49:49.358245Z"},"trusted":true},"outputs":[{"data":{"text/plain":["13583"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import BertForMaskedLM\n","# from torchsummary import summary\n","\n","pictobert = BertForMaskedLM.from_pretrained(\"/kaggle/input/pictobert-model/SemChilds_model\", output_hidden_states=True)\n","pictobert.config.vocab_size"]},{"cell_type":"markdown","metadata":{},"source":["# Model 1 : Experience Replay"]},{"cell_type":"markdown","metadata":{},"source":["* Model Overview:\n","   The model, LitBertClassifier, is built using PyTorch Lightning and utilizes a pretrained BERT-based architecture (BertForMaskedLM: PictoBERT) for next word prediction tasks.\n","   It includes an external memory buffer mechanism to store and sample data for continual learning.\n","   - The `memory buffer` is designed to store past training samples to facilitate experience replay (ER), a technique commonly used in continual learning.\n","   - ER helps `mitigate catastrophic forgetting`, a common problem in neural networks when they are trained incrementally on new tasks. \n","   - The buffer allows the model to remember and rehearse past data, effectively balancing learning between new and old data.\n","\n","\n","* Key Components:\n","\n","  - <b>BERT Architecture</b>: Uses a pre-trained BERT model (pictobert) to encode input text, which is further fine-tuned for specific tasks.\n","  - <b>Memory Buffer</b>: Implements an experience replay (ER) mechanism with a fixed memory size (`mem_sz = 1000`) to store and retrieve batches during training.\n","  - <b>Custom Memory Management</b>: Contains methods to initialize, update, sample from, and combine memory batches (<i>update_memory, sample_from_memory, combine_batches</i>).\n","\n","* Training and Optimization:\n","\n","  - <b>Optimizer</b>: Uses the `AdamW optimizer` with a polynomial decay learning rate scheduler (get_polynomial_decay_schedule_with_warmup).\n","  - <b>Freezing Layers</b>: Option to freeze specific layers of the BERT model for more efficient fine-tuning.\n","\n","* Evaluation:\n","  \n","  - The use of experience replay in the new model has resulted in a substantial `87.43% decrease in loss` and a `13.31% decrease in perplexity`, demonstrating significantly enhanced performance, indicating significantly better convergence and optimization.\n"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T15:19:45.857433Z","iopub.status.busy":"2024-08-25T15:19:45.856399Z","iopub.status.idle":"2024-08-25T15:19:47.034541Z","shell.execute_reply":"2024-08-25T15:19:47.033680Z","shell.execute_reply.started":"2024-08-25T15:19:45.857388Z"},"trusted":true},"outputs":[],"source":["import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","from torch.nn import functional as F\n","from sklearn.metrics import f1_score\n","from transformers import BertForMaskedLM, AdamW, BertTokenizer\n","from torch.utils.data import DataLoader, ConcatDataset\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from transformers import get_polynomial_decay_schedule_with_warmup\n","from scipy import stats\n","import numpy as np\n","\n","\n","class ER_PictoBERT(pl.LightningModule):\n","    def __init__(self, pretrained_model_name='bert-large-uncased', mem_sz=1000, alpha=1e-3, beta=1e-3):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.batch_size = 8\n","        self.lr = LEARNING_RATE\n","        self.validation_step_outputs = []\n","        self.validation_step_targets = []\n","        self.mem_sz = mem_sz\n","        self.ptr = 0  # Pointer to the current memory index\n","        self.size = 0  # Current size of the memory buffer\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.memory = {\n","            \"input_ids\": None,\n","            \"attention_mask\": None,\n","            \"labels\": None\n","        }  # Initialize the memory buffer for ER\n","        self.bert = pictobert\n","\n","\n","    def freeze_to(self, layers):\n","        for param in self.bert.bert.encoder.layer[:layers].parameters():\n","            param.requires_grad = False\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        # Check for invalid values in input_ids\n","        if torch.any(input_ids < 0) or torch.any(input_ids >= self.bert.config.vocab_size):\n","            print(self.bert.config.vocab_size)\n","            print(\"Invalid input_ids detected!\")\n","            print(\"Min value:\", torch.min(input_ids))\n","            print(\"Max value:\", torch.max(input_ids))\n","        if labels is None:\n","            output = self.bert(\n","                input_ids=input_ids,\n","                attention_mask=attention_mask,\n","            )\n","        output = self.bert(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","        return output\n","\n","    def combine_batches(self, Bn, BM):\n","            # Get the device of the current batch (Bn)\n","        device = Bn[\"input_ids\"].device\n","\n","        # Move BM (memory batch) to the same device as Bn if they are not already on the same device\n","        BM = {key: value.to(device) for key, value in BM.items()}\n","        \n","        return {\"input_ids\": torch.cat([Bn[\"input_ids\"], BM[\"input_ids\"]], dim=0),\n","                             \"attention_mask\": torch.cat([Bn[\"attention_mask\"], BM[\"attention_mask\"]], dim=0),\n","                             \"labels\": torch.cat([Bn[\"labels\"], BM[\"labels\"]], dim=0)}\n","\n","    def sample_from_memory(self, sample_size):\n","        if self.is_empty():\n","            return None\n","\n","        # Sample random indices from memory\n","        indices = torch.randperm(self.memory[\"input_ids\"].size(0))[:sample_size]\n","\n","        sampled_batch = {\n","            \"input_ids\": self.memory[\"input_ids\"][indices],\n","            \"attention_mask\": self.memory[\"attention_mask\"][indices],\n","            \"labels\": self.memory[\"labels\"][indices]\n","        }\n","        return sampled_batch\n","\n","    def training_step(self, batch, batch_idx):\n","        # Current batch from general training data\n","\n","        if self.is_empty():\n","            print(self.is_empty())\n","            combined_batch = batch\n","        # Combine current batch and personalized batch (with potential memory data)\n","        else : \n","            BM = self.sample_from_memory(min(len(self.memory), self.batch_size))\n","            combined_batch = self.combine_batches(batch, BM)\n","            \n","        # Train on the combined batches\n","        # for combined_batch in combined_dataloader:\n","        outputs = self._shared_step(combined_batch, batch_idx)\n","        loss = outputs[0]\n","        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n","\n","        # Update memory with personalized data\n","        self.update_memory(batch)\n","        return loss\n","\n","\n","    def meta_train_step(self, X, Y, W, alpha):\n","        self.train()\n","        optimizer = AdamW(W.parameters(), lr=alpha)\n","        optimizer.zero_grad()\n","        outputs = self.forward(X, attention_mask=None, labels=Y)\n","        loss = outputs.loss\n","        print(\"Meta-training loss:\", loss)\n","        loss.backward()\n","        optimizer.step()\n","        return W\n","\n","    def is_empty(self):\n","        return self.memory[\"input_ids\"] is None\n","    \n","    def update_memory(self, batch):\n","        batch_size = batch[\"input_ids\"].size(0)\n","\n","        if self.memory[\"input_ids\"] is None:\n","            # Pre-allocate memory buffer with fixed size\n","            self.memory[\"input_ids\"] = torch.zeros((self.mem_sz, *batch[\"input_ids\"].shape[1:]), dtype=batch[\"input_ids\"].dtype)\n","            self.memory[\"attention_mask\"] = torch.zeros((self.mem_sz, *batch[\"attention_mask\"].shape[1:]), dtype=batch[\"attention_mask\"].dtype)\n","            self.memory[\"labels\"] = torch.zeros((self.mem_sz, *batch[\"labels\"].shape[1:]), dtype=batch[\"labels\"].dtype)\n","\n","        # Calculate the end index for insertion\n","        end_ptr = (self.ptr + batch_size) % self.mem_sz\n","\n","        if end_ptr > self.ptr:\n","            # Case where we don't wrap around the buffer\n","            self.memory[\"input_ids\"][self.ptr:end_ptr] = batch[\"input_ids\"]\n","            self.memory[\"attention_mask\"][self.ptr:end_ptr] = batch[\"attention_mask\"]\n","            self.memory[\"labels\"][self.ptr:end_ptr] = batch[\"labels\"]\n","        else:\n","            # Case where we wrap around the buffer\n","            part1_len = self.mem_sz - self.ptr\n","            self.memory[\"input_ids\"][self.ptr:] = batch[\"input_ids\"][:part1_len]\n","            self.memory[\"attention_mask\"][self.ptr:] = batch[\"attention_mask\"][:part1_len]\n","            self.memory[\"labels\"][self.ptr:] = batch[\"labels\"][:part1_len]\n","            self.memory[\"input_ids\"][:end_ptr] = batch[\"input_ids\"][part1_len:]\n","            self.memory[\"attention_mask\"][:end_ptr] = batch[\"attention_mask\"][part1_len:]\n","            self.memory[\"labels\"][:end_ptr] = batch[\"labels\"][part1_len:]\n","\n","        # Update pointer and size\n","        self.ptr = end_ptr\n","        self.size = min(self.size + batch_size, self.mem_sz)\n","\n","\n","    def _shared_step(self, batch, batch_idx):\n","        input_ids = batch[\"input_ids\"]\n","        attention_mask = batch[\"attention_mask\"]\n","        labels = batch[\"labels\"]\n","        outputs = self.forward(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","        # print(\"Outputs:\",outputs)\n","        return outputs\n","\n","    def train_dataloader(self, train_dataset):\n","        train_dataloader = DataLoader(\n","            train_dataset,\n","            batch_size=self.batch_size,\n","            num_workers=NUM_WORKERS,\n","            pin_memory=True,\n","            collate_fn=data_collator,\n","            drop_last=True,\n","            shuffle=True\n","        )\n","        return train_dataloader\n","\n","    def validation_step(self, batch, batch_idx):\n","        with torch.no_grad():\n","            result = self._shared_step(batch, batch_idx)\n","            val_loss = result[0].detach()\n","            logits = result[1].detach()\n","\n","            predictions = torch.argmax(logits, dim=-1)\n","            labels = batch['labels']\n","\n","            self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True)\n","\n","            perplexity = torch.exp(val_loss)\n","            self.log(\"val_ppl\", perplexity, on_epoch=True, prog_bar=True)\n","\n","            # Save predictions and targets for F1 score calculation\n","            self.validation_step_outputs.append(predictions.cpu())  # Move to CPU to avoid memory issues\n","            self.validation_step_targets.append(labels.cpu())  # Move to CPU to avoid memory issues\n","\n","            # Return loss, labels and perplexity calculation\n","            return {\n","                \"val_loss\": val_loss,\n","                \"predictions\": predictions,\n","                \"labels\": labels\n","            }\n","\n","\n","    def on_validation_epoch_end(self):\n","        # Concatenate all predictions and targets\n","        all_preds = torch.cat(self.validation_step_outputs).numpy()\n","        all_targets = torch.cat(self.validation_step_targets).numpy()\n","\n","         # Calculate F1 score\n","        f1 = f1_score(all_targets.flatten(), all_preds.flatten(), average='weighted')\n","\n","        # Log F1 score\n","        self.log('val_f1', f1, on_epoch=True, prog_bar=True)\n","\n","        # Clear stored predictions and targets\n","        self.validation_step_outputs.clear()\n","        self.validation_step_targets.clear()\n","\n","\n","    def test_step(self, batch, batch_idx):\n","        with torch.no_grad():\n","            result = self._shared_step(batch, batch_idx)\n","            loss = result[0].detach()\n","            logits = result[1].detach()\n","\n","            predictions = torch.argmax(logits, dim=-1)\n","            labels = batch['labels']\n","\n","            perplexity = torch.exp(loss)\n","            self.log(\"test_ppl\", perplexity, on_epoch=True, prog_bar=True)\n","            self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n","\n","            return {\n","                \"test_ppl\": perplexity,\n","                \"test_loss\": loss\n","            }\n","\n","\n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n","        scheduler = {\n","            'scheduler': get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS,\n","                                                                   num_training_steps=MAX_EPOCHS, lr_end=1e-09),\n","            'name': 'lr'\n","        }\n","        return [optimizer], [scheduler]\n","\n","    def backward(self, loss):\n","        loss.backward()\n","\n","# Callbacks and logger setup\n","LOGS_PATH = \"./logs/ARASAAC-contextual-ft\"\n","CHECKPOINTS_PATH = \"./checkpoints/ARASAAC-contextual-ft\"\n","\n","tb_logger = TensorBoardLogger(LOGS_PATH, name='logger', version='version')\n","lr_monitor = LearningRateMonitor(logging_interval='epoch')\n","\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath=CHECKPOINTS_PATH,\n","    filename='bert-large-{epoch:02d}-{train_loss:.2f}-{val_loss:.2f}',\n","    mode='min',\n","    monitor=\"val_loss\",\n","    save_top_k=5\n",")\n","\n","trainer = pl.Trainer(\n","    max_epochs= MAX_EPOCHS,\n","    logger=tb_logger,\n","    callbacks=[checkpoint_callback, lr_monitor],\n","    precision=\"16-mixed\",\n","    accelerator=\"gpu\"\n",")\n","\n","\n","to_train = ER_PictoBERT()\n"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T15:19:47.240196Z","iopub.status.busy":"2024-08-25T15:19:47.239379Z","iopub.status.idle":"2024-08-25T18:39:35.146736Z","shell.execute_reply":"2024-08-25T18:39:35.145661Z","shell.execute_reply.started":"2024-08-25T15:19:47.240156Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1844be6235a344d596c6192fc0be14af","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["True\n"]},{"name":"stderr","output_type":"stream","text":["IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n","IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n","IOPub message rate exceeded.\n","The notebook server will temporarily stop sending output\n","to the client in order to avoid crashing it.\n","To change this limit, set the config variable\n","`--NotebookApp.iopub_msg_rate_limit`.\n","\n","Current values:\n","NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n","NotebookApp.rate_limit_window=3.0 (secs)\n","\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]}],"source":["trainer.fit(to_train, train_dataloader, val_dataloader)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T18:39:53.050697Z","iopub.status.busy":"2024-08-25T18:39:53.050038Z","iopub.status.idle":"2024-08-25T18:39:53.057928Z","shell.execute_reply":"2024-08-25T18:39:53.056988Z","shell.execute_reply.started":"2024-08-25T18:39:53.050658Z"},"trusted":true},"outputs":[{"data":{"text/markdown":["## ER Model: Validation Set Metrics"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Final val_ppl : 10.19471549987793\n","Final val_loss: 2.1363754272460938\n"]}],"source":["# After training\n","display_markdown(\"## ER Model: Validation Set Metrics\")\n","# print(f\"Final Training Loss: {trainer.logged_metrics[\"train_loss\"]}\")\n","print(f\"Final val_ppl : {trainer.logged_metrics['val_ppl']}\")\n","print(f\"Final val_loss: {trainer.logged_metrics['val_loss']}\")"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T18:40:12.170659Z","iopub.status.busy":"2024-08-25T18:40:12.169906Z","iopub.status.idle":"2024-08-25T18:40:30.144648Z","shell.execute_reply":"2024-08-25T18:40:30.143644Z","shell.execute_reply.started":"2024-08-25T18:40:12.170621Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d826e7447f15476aa9faa2a601682b70","version_major":2,"version_minor":0},"text/plain":["Testing: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.2029049396514893     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_ppl          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    10.837124824523926     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.2029049396514893    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_ppl         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   10.837124824523926    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"text/plain":["[{'test_ppl': 10.837124824523926, 'test_loss': 2.2029049396514893}]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["trainer.test(to_train, dataloaders=test_dataloader)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-08-25T18:40:34.665820Z","iopub.status.busy":"2024-08-25T18:40:34.664960Z","iopub.status.idle":"2024-08-25T18:40:53.047647Z","shell.execute_reply":"2024-08-25T18:40:53.046644Z","shell.execute_reply.started":"2024-08-25T18:40:34.665778Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"461cd406f16342f2acc330125c1678b9","version_major":2,"version_minor":0},"text/plain":["Testing: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     2.141904354095459     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_ppl          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    10.414925575256348     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    2.141904354095459    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_ppl         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   10.414925575256348    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'test_ppl': 10.414925575256348, 'test_loss': 2.141904354095459}]"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["trainer.test(to_train, dataloaders=test_dataloader)"]},{"cell_type":"markdown","metadata":{},"source":["# Model 2: Meta PictoBERT"]},{"cell_type":"markdown","metadata":{},"source":["+ <b>Base Model</b>: Utilizes the pre-trained BERT model (pictoBERT) designed for masked language modeling, which serves as the foundation for downstream tasks such as understanding and generating natural language text.\n","\n","+ <b>Meta-Learning Framework</b>: The model leverages meta-learning techniques to adapt quickly to new tasks by simulating a scenario where it learns from a smaller dataset (trajectory data) and then evaluates on another set (meta-test data). This approach is designed to improve generalization to new, unseen data.\n","  - using a meta-learning approach, the model can quickly adapt to new tasks, demonstrating strong generalization capabilities\n","\n","* Custom Layers:\n","\n","  - <b>Linear Layers</b>: Custom linear layers are added on top of BERT to learn specific task representations.\n","  - <b>LSTM Layer</b>: Provides optional recurrent neural network capability for capturing sequential dependencies in the data if needed.\n","  - <b>Custom Weights</b>: Implements a mechanism to apply custom weights dynamically, allowing for more fine-tuned control over the learning process.\n","\n","* Meta-Learning Update Mechanism:\n","\n","  1. <b>Inner Update</b>: \n","     + Implements an inner-loop optimization step where gradients are calculated and weights are updated using a `meta-learning rate (alpha)`, simulating multiple learning episodes within a single training step.\n","     + the inner loop updates a temporary set of model parameters, called `fast_weights`, using gradient descent based on a task-specific loss (cross-entropy loss). This step mimics learning on a small dataset from scratch or with few examples.\n","\n","  2. <b>Outer Loop (Meta-Learning)</b>:\n","     + The outer loop evaluates the performance of the inner-loop updates on new tasks (meta-test data). It aims to find an optimal initialization or model configuration that, when fine-tuned, can quickly adapt to various tasks.\n","     + The Meta_PictoBERT model computes a `meta-loss` using the parameters learned in the inner loop (fast_weights) and updates the main model parameters to improve the ability to generalize across tasks.\n","\n","* Optimization Strategy:\n","\n","  - <b>Optimizer</b>: Uses the `AdamW optimizer`, which is well-suited for training large models like BERT due to its adaptive learning rate and weight decay capabilities.\n","  - <b>Learning Rate Scheduler</b>: Utilizes a polynomial decay schedule with warmup to adjust the learning rate dynamically during training, which helps prevent the model from overshooting the optimal solution and ensures steady convergence.\n","\n","* Evaluation:\n","  \n","  - The use of meta learning, resulted in a substantial `decrease in loss` and a `lower perplexity`, even with `training for only 3 epochs` (due to memory constraints).  \n","  - The use of meta-learning in the Meta_PictoBERT model enables it to rapidly adapt to new tasks with minimal data, improve generalization across diverse domains, and retain knowledge effectively over time. This approach provides significant advantages in environments with continuous learning requirements."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T12:28:40.741403Z","iopub.status.busy":"2024-08-28T12:28:40.740101Z","iopub.status.idle":"2024-08-28T12:28:41.930618Z","shell.execute_reply":"2024-08-28T12:28:41.929645Z","shell.execute_reply.started":"2024-08-28T12:28:40.741358Z"},"trusted":true},"outputs":[],"source":["import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","from torch.nn import functional as F\n","from sklearn.metrics import f1_score\n","from transformers import BertForMaskedLM, AdamW, BertTokenizer\n","from torch.utils.data import DataLoader, ConcatDataset\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from transformers import get_polynomial_decay_schedule_with_warmup\n","from scipy import stats\n","import numpy as np\n","from pytorch_lightning.callbacks import RichProgressBar\n","\n","\n","class Meta_PictoBERT(pl.LightningModule):\n","    def __init__(self, pretrained_model_name='bert-large-uncased',num_lstm_layers: int = 4,\n","                 lstm_hidden_size: int = 128,\n","                 ffnn_hidden_size: int = 64, mem_sz=1000, alpha=3e-4, beta=1e-3):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.batch_size = 32\n","        self.lr = LEARNING_RATE\n","        self.train_dataset = train_dataset  # Global training dataset\n","        self.mem_sz = mem_sz\n","        self.alpha = alpha\n","        self.meta_lr = beta\n","        self.validation_step_outputs = []\n","        self.validation_step_targets = []\n","        self.memory = []  # Initialize the memory buffer for ER\n","        self.bert = pictobert #BertForMaskedLM.from_pretrained(pretrained_model_name)\n","        \n","        #self.lstm = nn.LSTM(input_size=self.bert.config.hidden_size,\n","#                             hidden_size=lstm_hidden_size,\n","#                             num_layers=num_lstm_layers,\n","#                             batch_first=True,\n","#                             bidirectional=True)\n","        \n","        self.W = nn.Sequential(\n","            nn.Linear(self.bert.config.hidden_size, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, self.bert.config.vocab_size)\n","        )\n","\n","    def freeze_to(self, layers):\n","        for param in self.bert.bert.encoder.layer[:layers].parameters():\n","            param.requires_grad = False\n","\n","\n","    def forward(self, input_ids, attention_mask, fast_weights = None, labels=None, train = False):\n","        # Check for invalid values in input_ids\n","        if torch.any(input_ids < 0) or torch.any(input_ids >= self.bert.config.vocab_size):\n","            print(self.bert.config.vocab_size)\n","            print(\"Invalid input_ids detected!\")\n","            print(\"Min value:\", torch.min(input_ids))\n","            print(\"Max value:\", torch.max(input_ids))\n","        # Forward pass through BERT\n","        bert_output = self.bert(input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels = labels).hidden_states[-1]\n","        \n","#         lstm_output,_ = self.lstm(bert_output)\n","        \n","        # If no fast_weights are provided, use the model's weights\n","        if train :\n","            logits = self.apply_custom_weights(bert_output, fast_weights)\n","            # return logits, bert_output[0]\n","        else :\n","            logits = self.W(bert_output)\n","\n","        return logits\n","\n","\n","    def inner_update(self, x, att_mask, fast_weights, y):\n","        if fast_weights is None:\n","            fast_weights = list(self.W.parameters())\n","\n","        # Forward pass through BERT and meta model using fast_weights\n","        logits = self.forward(x, att_mask, fast_weights, y, train =True)\n","\n","        loss = F.cross_entropy(logits.view(-1, self.bert.config.vocab_size), y.view(-1), ignore_index=-100)\n","        grad = torch.autograd.grad(loss, fast_weights, create_graph=True)\n","\n","        new_weights = []\n","        for param, g in zip(fast_weights, grad):\n","            new_weights.append(param - self.alpha * g)\n","\n","        return new_weights\n","\n","    def apply_custom_weights(self, x, weights):\n","        idx = 0\n","        for name, layer in self.W.named_modules():\n","            if isinstance(layer, nn.Linear):\n","                w, b = weights[idx], weights[idx + 1]\n","                x = F.linear(x, w, b)\n","                idx += 2\n","            elif isinstance(layer, nn.Conv2d):\n","                w, b = weights[idx], weights[idx + 1]\n","                x = F.conv2d(x, w, b, stride=layer.stride, padding=layer.padding)\n","                idx += 2\n","            elif isinstance(layer, nn.ReLU):\n","                x = F.relu(x)\n","\n","        return x\n","\n","    def meta_loss(self, x, att_mask, fast_weights, y):\n","\n","        # Forward pass through BERT and meta model using fast_weights\n","        logits = self.forward(x, att_mask, fast_weights, y, train =True)\n","\n","        loss = F.cross_entropy(logits.view(-1, self.bert.config.vocab_size), y.view(-1), ignore_index=-100)\n","\n","        return loss, logits\n","\n","    def eval_accuracy(self, logits, y):\n","        pred_q = F.softmax(logits, dim=1).argmax(dim=1)\n","        correct = torch.eq(pred_q, y).sum().item()\n","        return correct\n","\n","   \n","    def training_step(self, batch, batch_idx):\n","        X, Y = batch['input_ids'], batch['labels']\n","        mask = batch['attention_mask']\n","\n","        # Perform a single random sampling and split into trajectory and meta-test\n","        total_indices = list(range(len(X)))\n","        # random.shuffle(total_indices)\n","\n","        # Split indices: 50% for trajectory, 50% for meta-test (adjust the split ratio if needed)\n","        split_idx = len(X) // 2\n","        traj_indices = total_indices[:split_idx]\n","        meta_test_indices = total_indices[split_idx:]\n","\n","        # Get trajectory and meta-test data\n","        X_traj = X[traj_indices]\n","        mask_traj = mask[traj_indices]\n","        Y_traj = Y[traj_indices]\n","\n","        X_meta = X[meta_test_indices]\n","        mask_meta = mask[meta_test_indices]\n","        Y_meta = Y[meta_test_indices]\n","\n","        # Meta-learning loop\n","        fast_weights = None\n","        # for j in range(len(X_traj)):\n","        #     fast_weights = self.inner_update(X_traj[j].unsqueeze(0), mask_traj[j].unsqueeze(0), fast_weights, Y_traj[j].unsqueeze(0))\n","        fast_weights = self.inner_update(X_traj, mask_traj, fast_weights, Y_traj)\n","\n","        # Compute meta-loss on meta-test set\n","        meta_loss, _ = self.meta_loss(X_meta, mask_meta, fast_weights, Y_meta)\n","\n","        self.log(\"train_loss\", meta_loss, on_epoch=True, prog_bar=True)\n","        return meta_loss\n","\n","    def _shared_step(self, batch, batch_idx):\n","        input_ids = batch[\"input_ids\"]\n","        attention_mask = batch[\"attention_mask\"]\n","        labels = batch[\"labels\"]\n","        logits = self.forward(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","        loss = F.cross_entropy(logits.view(-1, self.bert.config.vocab_size), labels.view(-1), ignore_index=-100)\n","        return loss, logits\n","\n","    def train_dataloader(self):\n","        train_dataloader = DataLoader(\n","            self.train_dataset,\n","            batch_size=self.batch_size,\n","            num_workers=NUM_WORKERS,\n","            pin_memory=True,\n","            collate_fn=data_collator,\n","            drop_last=True,\n","            shuffle=True\n","        )\n","        return train_dataloader\n","\n","    def validation_step(self, batch, batch_idx):\n","        with torch.no_grad():\n","            result = self._shared_step(batch, batch_idx)\n","            val_loss = result[0].detach()\n","            logits = result[1].detach()\n","\n","            predictions = torch.argmax(logits, dim=-1)\n","            labels = batch['labels']\n","\n","            self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True)\n","            \n","            perplexity = torch.exp(val_loss)\n","            self.log(\"val_ppl\", perplexity, on_epoch=True, prog_bar=True)\n","\n","            # Save predictions and targets for F1 score calculation\n","            self.validation_step_outputs.append(predictions.cpu())  # Move to CPU to avoid memory issues\n","            self.validation_step_targets.append(labels.cpu())  # Move to CPU to avoid memory issues\n","\n","            # Return loss and perplexity calculation\n","            return {\n","                \"val_loss\": val_loss,\n","                \"val_ppl\":  perplexity\n","            }\n","\n","    def on_validation_epoch_end(self):\n","\n","        # Concatenate all predictions and targets\n","        all_preds = torch.cat(self.validation_step_outputs).numpy()\n","        all_targets = torch.cat(self.validation_step_targets).numpy()\n","\n","        # Calculate F1 score\n","        f1 = f1_score(all_targets.flatten(), all_preds.flatten(), average='weighted')\n","\n","        # Log F1 score\n","        self.log('val_f1', f1, on_epoch=True, prog_bar=True)\n","\n","        # Clear stored predictions and targets\n","        self.validation_step_outputs.clear()\n","        self.validation_step_targets.clear()\n","\n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n","        scheduler = {\n","            'scheduler': get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS,\n","                                                                   num_training_steps=MAX_EPOCHS, lr_end=1e-09),\n","            'name': 'lr'\n","        }\n","        return [optimizer], [scheduler]\n","\n","    def test_step(self, batch, batch_idx):\n","        with torch.no_grad():\n","            result = self._shared_step(batch, batch_idx)\n","            loss = result[0].detach()\n","            logits = result[1].detach()\n","\n","            predictions = torch.argmax(logits, dim=-1)\n","            labels = batch['labels']\n","\n","            perplexity = torch.exp(loss)\n","            self.log(\"test_ppl\", perplexity, on_epoch=True, prog_bar=True)\n","            self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n","\n","\n","            return {\n","                \"test_ppl\": perplexity,\n","                \"test_loss\": loss\n","            }\n","\n","    def backward(self, loss):\n","        loss.backward()\n","\n","# # Callbacks and logger setup\n","# LOGS_PATH = \"./logs/ARASAAC-contextual-ft\"\n","# CHECKPOINTS_PATH = \"./checkpoints/ARASAAC-contextual-ft\"\n","\n","# tb_logger = TensorBoardLogger(LOGS_PATH, name='logger', version='version')\n","# lr_monitor = LearningRateMonitor(logging_interval='epoch')\n","\n","# checkpoint_callback = ModelCheckpoint(\n","#     dirpath=CHECKPOINTS_PATH,\n","#     filename='bert-large-{epoch:02d}-{train_loss:.2f}-{val_loss:.2f}',\n","#     mode='min',\n","#     monitor=\"val_loss\",\n","#     save_top_k=5\n","# )\n","\n","trainer = pl.Trainer(\n","    max_epochs=10,\n","    logger=False,\n","    callbacks=False, #[checkpoint_callback, lr_monitor],\n","    precision=\"16-mixed\",\n","    accelerator=\"gpu\"\n",")\n","\n","\n","to_train = Meta_PictoBERT()\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T12:28:48.921655Z","iopub.status.busy":"2024-08-28T12:28:48.921249Z","iopub.status.idle":"2024-08-28T18:19:10.558656Z","shell.execute_reply":"2024-08-28T18:19:10.557626Z","shell.execute_reply.started":"2024-08-28T12:28:48.921616Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d583ec98ea3c4ce6b89f7381491424b0","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]}],"source":["trainer.fit(to_train, train_dataloader, val_dataloader)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-28T18:19:10.560899Z","iopub.status.busy":"2024-08-28T18:19:10.560546Z","iopub.status.idle":"2024-08-28T18:19:28.943702Z","shell.execute_reply":"2024-08-28T18:19:28.942628Z","shell.execute_reply.started":"2024-08-28T18:19:10.560864Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22fe31ea6be843cb9f1e2a26f92a52f8","version_major":2,"version_minor":0},"text/plain":["Testing: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     3.559744358062744     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_ppl          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     43.75568389892578     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    3.559744358062744    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_ppl         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    43.75568389892578    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"text/plain":["[{'test_ppl': 43.75568389892578, 'test_loss': 3.559744358062744}]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["trainer.test(to_train, dataloaders=test_dataloader)"]},{"cell_type":"markdown","metadata":{},"source":["# Model 3: Meta_ER PictoBERT"]},{"cell_type":"markdown","metadata":{},"source":["1. <b>Core Components of MetaER_pictobert</b>:\n","\n","   - <b>Pre-trained BERT for Masked Language Modeling (pictobert)</b>: The model utilizes a pre-trained BERT (pictobert) as its base for encoding text. This provides a powerful foundation of language understanding, which can be fine-tuned for specific tasks.\n","   - <b>Experience Replay (ER) Memory Buffer</b>:\n","     + A memory buffer stores past examples from the personalized dataset to continuously retain and replay them during training. This helps the model retain knowledge over time.\n","     + <i>Memory Buffer Details</i>: The buffer uses a fixed size (`mem_sz = 1000`) and is updated regularly, replacing old samples with new ones as the buffer reaches capacity.\n","\n","   - <b>Meta-Learning Loop (MAML-Inspired)</b>:\n","     + The meta-learning loop enables the model to learn how to adapt quickly to new tasks by simulating learning on small datasets (<i>support set</i>) and evaluating on new examples (<i>query set</i>).\n","     + <b>Inner Loop (Task-Specific Learning)</b>: Updates fast weights (task-specific parameters) using a small subset of data. This loop mimics how the model would learn a new task from limited examples.\n","     + <b>Outer Loop (Meta-Optimization)</b>: Evaluates the effectiveness of the inner loop’s updates and adjusts the base model parameters (MetaER_pictobert) to enhance adaptability across tasks.\n","   - <b>Custom Weight Application</b>: A mechanism to dynamically apply custom weights for each layer during the meta-learning process, allowing for flexible updates and fine-tuning.\n","   - <b>Dynamic Learning Rate and Optimizer</b>: Uses `AdamW optimizer` with `polynomial decay scheduling` to fine-tune the learning process, ensuring better convergence over time.\n","\n","2. <b>Model Training Strategy</b>:\n","\n","   <b>Experience Replay and Meta-Learning Combined</b>:\n","     + <b>Combining Mini-Batches</b>: The model dynamically `combines current mini-batches with samples drawn from the memory buffer`, ensuring that the model is trained on both new and historical data. This balances learning between recent and past knowledge.\n","     + <b>Dynamic Sampling</b>: During training, a `random sampling process` splits data into two halves: a trajectory set for inner-loop meta-updates and a meta-test set for evaluating outer-loop optimization. This dual-phase training optimizes both specific and generalized learning capabilities.\n"," \n","3. <b>Memory Buffer Management</b>:\n","\n","   The memory buffer uses a `pointer mechanism (ptr)` to manage storage efficiently, allowing for the `cyclic replacement` of old examples. This mechanism ensures that the model maintains a diverse set of training examples without increasing memory usage indefinitely.\n","\n","4. <b>Custom Forward Pass and Inner Update</b>:\n","\n","   - The forward method allows flexibility by applying different layers and their weights dynamically, enabling more granular control over the learning process.\n","   - <b>Inner Update Mechanism</b>: Calculates the gradients using `torch.autograd.grad`, which creates graph-based gradients and allows the model to update itself in a task-specific manner, optimizing the fast weights for quick adaptation.\n","   - <b>Training Step</b>: Involves a combination of experience replay and meta-learning. After a batch is processed, the loss is calculated and used to update both the model weights and the memory buffer.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:51:10.798127Z","iopub.status.busy":"2024-08-29T18:51:10.797772Z","iopub.status.idle":"2024-08-29T18:51:10.929618Z","shell.execute_reply":"2024-08-29T18:51:10.928669Z","shell.execute_reply.started":"2024-08-29T18:51:10.798089Z"},"trusted":true},"outputs":[],"source":["import random\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","from torch.nn import functional as F\n","from sklearn.metrics import f1_score\n","from transformers import BertForMaskedLM, AdamW, BertTokenizer\n","from torch.utils.data import DataLoader, ConcatDataset\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n","from pytorch_lightning.loggers import TensorBoardLogger\n","from transformers import get_polynomial_decay_schedule_with_warmup\n","from scipy import stats\n","import numpy as np\n","from pytorch_lightning.callbacks import RichProgressBar\n","\n","\n","class MetaER_pictobert(pl.LightningModule):\n","    def __init__(self, pretrained_model_name='bert-large-uncased',num_lstm_layers: int = 4,\n","                 lstm_hidden_size: int = 128,\n","                 ffnn_hidden_size: int = 64, mem_sz=1000, alpha=3e-4, beta=1e-3):\n","        super().__init__()\n","        self.save_hyperparameters()\n","        self.batch_size = 8\n","        self.lr = LEARNING_RATE\n","        self.train_dataset = train_dataset  # Global training dataset\n","        self.mem_sz = mem_sz\n","        self.ptr = 0  # Pointer to the current memory index\n","        self.size = 0  # Current size of the memory buffer\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.memory = {\n","            \"input_ids\": None,\n","            \"attention_mask\": None,\n","            \"labels\": None\n","        } \n","        self.validation_step_outputs = []\n","        self.validation_step_targets = []\n","        self.bert = pictobert #BertForMaskedLM.from_pretrained(pretrained_model_name)\n","        \n","        #self.lstm = nn.LSTM(input_size=self.bert.config.hidden_size,\n","#                             hidden_size=lstm_hidden_size,\n","#                             num_layers=num_lstm_layers,\n","#                             batch_first=True,\n","#                             bidirectional=True)\n","        \n","        self.W = nn.Sequential(\n","            nn.Linear(self.bert.config.hidden_size, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, self.bert.config.vocab_size)\n","        )\n","\n","    def freeze_to(self, layers):\n","        for param in self.bert.bert.encoder.layer[:layers].parameters():\n","            param.requires_grad = False\n","\n","\n","    def forward(self, input_ids, attention_mask, fast_weights = None, labels=None, train = False):\n","        # Check for invalid values in input_ids\n","        if torch.any(input_ids < 0) or torch.any(input_ids >= self.bert.config.vocab_size):\n","            print(self.bert.config.vocab_size)\n","            print(\"Invalid input_ids detected!\")\n","            print(\"Min value:\", torch.min(input_ids))\n","            print(\"Max value:\", torch.max(input_ids))\n","        # Forward pass through BERT\n","        bert_output = self.bert(input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels = labels).hidden_states[-1]\n","        \n","#         lstm_output,_ = self.lstm(bert_output)\n","        \n","        # If no fast_weights are provided, use the model's weights\n","        if train :\n","            logits = self.apply_custom_weights(bert_output, fast_weights)\n","            # return logits, bert_output[0]\n","        else :\n","            logits = self.W(bert_output)\n","\n","        return logits\n","\n","\n","    def inner_update(self, x, att_mask, fast_weights, y):\n","        if fast_weights is None:\n","            fast_weights = list(self.W.parameters())\n","\n","\n","        # Forward pass through BERT and meta model using fast_weights\n","        logits = self.forward(x, att_mask, fast_weights, y, train =True)\n","\n","        loss = F.cross_entropy(logits.view(-1, self.bert.config.vocab_size), y.view(-1), ignore_index=-100)\n","        grad = torch.autograd.grad(loss, fast_weights, create_graph=True)\n","\n","        new_weights = []\n","        for param, g in zip(fast_weights, grad):\n","            new_weights.append(param - self.alpha * g)\n","\n","        return new_weights\n","\n","    def apply_custom_weights(self, x, weights):\n","        idx = 0\n","        for name, layer in self.W.named_modules():\n","            if isinstance(layer, nn.Linear):\n","                w, b = weights[idx], weights[idx + 1]\n","                x = F.linear(x, w, b)\n","                idx += 2\n","            elif isinstance(layer, nn.Conv2d):\n","                w, b = weights[idx], weights[idx + 1]\n","                x = F.conv2d(x, w, b, stride=layer.stride, padding=layer.padding)\n","                idx += 2\n","            elif isinstance(layer, nn.ReLU):\n","                x = F.relu(x)\n","            # Add more conditions if you have other types of layers\n","        return x\n","\n","    def meta_loss(self, x, att_mask, fast_weights, y):\n","\n","        # Forward pass through BERT and meta model using fast_weights\n","        logits = self.forward(x, att_mask, fast_weights, y, train =True)\n","\n","        loss = F.cross_entropy(logits.view(-1, self.bert.config.vocab_size), y.view(-1), ignore_index=-100)\n","        # logits = self.W(self.bert(x, params=fast_weights)[0])\n","        # loss_q = F.cross_entropy(logits, y)\n","        return loss, logits\n","\n","    def eval_accuracy(self, logits, y):\n","        pred_q = F.softmax(logits, dim=1).argmax(dim=1)\n","        correct = torch.eq(pred_q, y).sum().item()\n","        return correct\n","\n","    \n","    def combine_batches(self, Bn, BM):\n","            # Get the device of the current batch (Bn)\n","        device = Bn[\"input_ids\"].device\n","\n","        # Move BM (memory batch) to the same device as Bn if they are not already on the same device\n","        BM = {key: value.to(device) for key, value in BM.items()}\n","        \n","        return {\"input_ids\": torch.cat([Bn[\"input_ids\"], BM[\"input_ids\"]], dim=0),\n","                             \"attention_mask\": torch.cat([Bn[\"attention_mask\"], BM[\"attention_mask\"]], dim=0),\n","                             \"labels\": torch.cat([Bn[\"labels\"], BM[\"labels\"]], dim=0)}\n","\n","    def sample_from_memory(self, sample_size):\n","        if self.is_empty():\n","            return None\n","\n","        # Sample random indices from memory\n","        indices = torch.randperm(self.memory[\"input_ids\"].size(0))[:sample_size]\n","\n","        sampled_batch = {\n","            \"input_ids\": self.memory[\"input_ids\"][indices],\n","            \"attention_mask\": self.memory[\"attention_mask\"][indices],\n","            \"labels\": self.memory[\"labels\"][indices]\n","        }\n","        return sampled_batch\n","\n","    def training_step(self, batch, batch_idx):\n","        \n","        if self.is_empty():\n","            print(self.is_empty())\n","            combined_batch = batch\n","        # Experience Replay with personalized data\n","        else : \n","            BM = self.sample_from_memory(min(len(self.memory), self.batch_size))\n","            combined_batch = self.combine_batches(batch, BM)\n","        \n","        X, Y = combined_batch['input_ids'], combined_batch['labels']\n","        mask = combined_batch['attention_mask']\n","\n","        # Perform a single random sampling and split into trajectory and meta-test\n","        total_indices = list(range(len(X)))\n","        # random.shuffle(total_indices)\n","\n","        # Split indices: 50% for trajectory, 50% for meta-test (adjust the split ratio if needed)\n","        split_idx = len(X) // 2\n","        traj_indices = total_indices[:split_idx]\n","        meta_test_indices = total_indices[split_idx:]\n","\n","        # Get trajectory and meta-test data\n","        X_traj = X[traj_indices]\n","        mask_traj = mask[traj_indices]\n","        Y_traj = Y[traj_indices]\n","\n","        X_meta = X[meta_test_indices]\n","        mask_meta = mask[meta_test_indices]\n","        Y_meta = Y[meta_test_indices]\n","\n","        # Meta-learning loop\n","        fast_weights = None\n","        # for j in range(len(X_traj)):\n","        #     fast_weights = self.inner_update(X_traj[j].unsqueeze(0), mask_traj[j].unsqueeze(0), fast_weights, Y_traj[j].unsqueeze(0))\n","        fast_weights = self.inner_update(X_traj, mask_traj, fast_weights, Y_traj)\n","\n","        # Compute meta-loss on meta-test set\n","        meta_loss, _ = self.meta_loss(X_meta, mask_meta, fast_weights, Y_meta)\n","\n","        # Update memory buffer\n","        self.log(\"train_loss\", meta_loss, on_epoch=True, prog_bar=True)\n","        \n","        self.update_memory(batch)\n","        \n","        return meta_loss\n","\n","    def is_empty(self):\n","        return self.memory[\"input_ids\"] is None\n","    \n","    def update_memory(self, batch):\n","        batch_size = batch[\"input_ids\"].size(0)\n","\n","        if self.memory[\"input_ids\"] is None:\n","            # Pre-allocate memory buffer with fixed size\n","            self.memory[\"input_ids\"] = torch.zeros((self.mem_sz, *batch[\"input_ids\"].shape[1:]), dtype=batch[\"input_ids\"].dtype)\n","            self.memory[\"attention_mask\"] = torch.zeros((self.mem_sz, *batch[\"attention_mask\"].shape[1:]), dtype=batch[\"attention_mask\"].dtype)\n","            self.memory[\"labels\"] = torch.zeros((self.mem_sz, *batch[\"labels\"].shape[1:]), dtype=batch[\"labels\"].dtype)\n","\n","        # Calculate the end index for insertion\n","        end_ptr = (self.ptr + batch_size) % self.mem_sz\n","\n","        if end_ptr > self.ptr:\n","            # Case where we don't wrap around the buffer\n","            self.memory[\"input_ids\"][self.ptr:end_ptr] = batch[\"input_ids\"]\n","            self.memory[\"attention_mask\"][self.ptr:end_ptr] = batch[\"attention_mask\"]\n","            self.memory[\"labels\"][self.ptr:end_ptr] = batch[\"labels\"]\n","        else:\n","            # Case where we wrap around the buffer\n","            part1_len = self.mem_sz - self.ptr\n","            self.memory[\"input_ids\"][self.ptr:] = batch[\"input_ids\"][:part1_len]\n","            self.memory[\"attention_mask\"][self.ptr:] = batch[\"attention_mask\"][:part1_len]\n","            self.memory[\"labels\"][self.ptr:] = batch[\"labels\"][:part1_len]\n","            self.memory[\"input_ids\"][:end_ptr] = batch[\"input_ids\"][part1_len:]\n","            self.memory[\"attention_mask\"][:end_ptr] = batch[\"attention_mask\"][part1_len:]\n","            self.memory[\"labels\"][:end_ptr] = batch[\"labels\"][part1_len:]\n","\n","        # Update pointer and size\n","        self.ptr = end_ptr\n","        self.size = min(self.size + batch_size, self.mem_sz)\n","\n","    def _shared_step(self, batch, batch_idx):\n","        input_ids = batch[\"input_ids\"]\n","        attention_mask = batch[\"attention_mask\"]\n","        labels = batch[\"labels\"]\n","        logits = self.forward(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","        loss = F.cross_entropy(logits.view(-1, self.bert.config.vocab_size), labels.view(-1), ignore_index=-100)\n","        return loss, logits\n","\n","    def train_dataloader(self):\n","        train_dataloader = DataLoader(\n","            self.train_dataset,\n","            batch_size=self.batch_size,\n","            num_workers=NUM_WORKERS,\n","            pin_memory=True,\n","            collate_fn=data_collator,\n","            drop_last=True,\n","            shuffle=True\n","        )\n","        return train_dataloader\n","\n","    def validation_step(self, batch, batch_idx):\n","        with torch.no_grad():\n","            result = self._shared_step(batch, batch_idx)\n","            val_loss = result[0].detach()\n","            logits = result[1].detach()\n","\n","            predictions = torch.argmax(logits, dim=-1)\n","            labels = batch['labels']\n","\n","            self.log(\"val_loss\", val_loss, on_epoch=True, prog_bar=True)\n","            \n","            perplexity = torch.exp(val_loss)\n","            self.log(\"val_ppl\", perplexity, on_epoch=True, prog_bar=True)\n","\n","            # Save predictions and targets for F1 score calculation\n","            self.validation_step_outputs.append(predictions.cpu())  # Move to CPU to avoid memory issues\n","            self.validation_step_targets.append(labels.cpu())  # Move to CPU to avoid memory issues\n","\n","            # Return loss and perplexity calculation\n","            return {\n","                \"val_loss\": val_loss,\n","                \"val_ppl\":  perplexity\n","            }\n","\n","    def on_validation_epoch_end(self):\n","\n","        # Concatenate all predictions and targets\n","        all_preds = torch.cat(self.validation_step_outputs).numpy()\n","        all_targets = torch.cat(self.validation_step_targets).numpy()\n","\n","        # Calculate F1 score\n","        f1 = f1_score(all_targets.flatten(), all_preds.flatten(), average='weighted')\n","\n","\n","        # Log F1 score\n","        self.log('val_f1', f1, on_epoch=True, prog_bar=True)\n","\n","        # Clear stored predictions and targets\n","        self.validation_step_outputs.clear()\n","        self.validation_step_targets.clear()\n","\n","    def configure_optimizers(self):\n","        optimizer = AdamW(self.parameters(), lr=self.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n","        scheduler = {\n","            'scheduler': get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS,\n","                                                                   num_training_steps=MAX_EPOCHS, lr_end=1e-09),\n","            'name': 'lr'\n","        }\n","        return [optimizer], [scheduler]\n","\n","    def test_step(self, batch, batch_idx):\n","        with torch.no_grad():\n","            result = self._shared_step(batch, batch_idx)\n","            loss = result[0].detach()\n","            logits = result[1].detach()\n","\n","            predictions = torch.argmax(logits, dim=-1)\n","            labels = batch['labels']\n","\n","            perplexity = torch.exp(loss)\n","            self.log(\"test_ppl\", perplexity, on_epoch=True, prog_bar=True)\n","            self.log(\"test_loss\", loss, on_epoch=True, prog_bar=True)\n","\n","\n","            return {\n","                \"test_ppl\": perplexity,\n","                \"test_loss\": loss\n","            }\n","\n","    def backward(self, loss):\n","        loss.backward()\n","\n","\n","trainer = pl.Trainer(\n","    max_epochs=MAX_EPOCHS,\n","    logger=False,\n","    callbacks=False, #[checkpoint_callback, lr_monitor],\n","    precision=\"16-mixed\",\n","    accelerator=\"gpu\"\n",")\n","\n","\n","to_train = MetaER_pictobert()\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T18:51:11.859151Z","iopub.status.busy":"2024-08-29T18:51:11.858514Z","iopub.status.idle":"2024-08-29T22:26:43.166099Z","shell.execute_reply":"2024-08-29T22:26:43.165197Z","shell.execute_reply.started":"2024-08-29T18:51:11.859110Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e83b3b078e314cd9bc497d3795460d9a","version_major":2,"version_minor":0},"text/plain":["Training: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["True\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n","/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  self.pid = os.fork()\n"]}],"source":["trainer.fit(to_train, train_dataloader, val_dataloader)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T22:27:37.094354Z","iopub.status.busy":"2024-08-29T22:27:37.093672Z","iopub.status.idle":"2024-08-29T22:27:37.102340Z","shell.execute_reply":"2024-08-29T22:27:37.101419Z","shell.execute_reply.started":"2024-08-29T22:27:37.094309Z"},"trusted":true},"outputs":[{"data":{"text/markdown":["## MetaER Model: Validation Set Metrics"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Final val_ppl : 49.16434097290039\n","Final val_loss: 3.6612133979797363\n"]}],"source":["# After training\n","display_markdown(\"## MetaER Model: Validation Set Metrics\")\n","# print(f\"Final Training Loss: {trainer.logged_metrics[\"train_loss\"]}\")\n","print(f\"Final val_ppl : {trainer.logged_metrics['val_ppl']}\")\n","print(f\"Final val_loss: {trainer.logged_metrics['val_loss']}\")"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T22:28:03.523639Z","iopub.status.busy":"2024-08-29T22:28:03.522639Z","iopub.status.idle":"2024-08-29T22:28:22.035324Z","shell.execute_reply":"2024-08-29T22:28:22.034246Z","shell.execute_reply.started":"2024-08-29T22:28:03.523595Z"},"trusted":true},"outputs":[{"data":{"text/markdown":["## MetaER Model: Test Set Metrics"],"text/plain":["<IPython.core.display.Markdown object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"767bafb896264803983ce350ff82c6e1","version_major":2,"version_minor":0},"text/plain":["Testing: |          | 0/? [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.6606292724609375     </span>│\n","│<span style=\"color: #008080; text-decoration-color: #008080\">         test_ppl          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     48.77863311767578     </span>│\n","└───────────────────────────┴───────────────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n","│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.6606292724609375    \u001b[0m\u001b[35m \u001b[0m│\n","│\u001b[36m \u001b[0m\u001b[36m        test_ppl         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    48.77863311767578    \u001b[0m\u001b[35m \u001b[0m│\n","└───────────────────────────┴───────────────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[{'test_ppl': 48.77863311767578, 'test_loss': 3.6606292724609375}]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["display_markdown(\"## MetaER Model: Test Set Metrics\")\n","trainer.test(to_train, dataloaders=test_dataloader)"]},{"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-08-29T22:36:15.982842Z","iopub.status.busy":"2024-08-29T22:36:15.982135Z","iopub.status.idle":"2024-08-29T22:36:16.250083Z","shell.execute_reply":"2024-08-29T22:36:16.249074Z","shell.execute_reply.started":"2024-08-29T22:36:15.982801Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['do%2:41:01::',\n"," 'go_to%2:42:00::',\n"," 'go%2:38:00::',\n"," 'not%4:02:00::',\n"," 'nice%3:00:00::',\n"," 'be%2:42:06::',\n"," 'will%2:32:00::',\n"," 'like',\n"," 'at',\n"," 'know%2:31:01::']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["import torch.nn.functional as F\n","def get_top_k(sentence, k):\n","    text = \" \".join(sentence+['[MASK]','.'])\n","  # return text\n","    tokenized = loaded_tokenizer(text, return_tensors=\"pt\")\n","\n","    input_ids, attention_mask = tokenized['input_ids'], tokenized['attention_mask']\n","  # input_ids = tensor([loaded_tokenizer.convert_tokens_to_ids(sentence+['[MASK]','.'])])\n","    outputs = to_train.forward(input_ids, attention_mask)\n","    predictions = F.softmax(outputs, dim=-1)\n","\n","    mask_idx = input_ids.tolist()[0].index(loaded_tokenizer.mask_token_id)\n","    probs = predictions[0, mask_idx, :]\n","    return loaded_tokenizer.convert_ids_to_tokens(probs.topk(k)[1])\n","\n","get_top_k(['mommy%1:18:00::', 'be%2:42:03::'],10)"]},{"cell_type":"markdown","metadata":{},"source":["# How Meta_ER PictoBERT Helps in the Continual Learning Task for Personalized Pictogram Recommendations:"]},{"cell_type":"markdown","metadata":{},"source":["A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting.\n","\n","The combined approach of MAML and Experience Replay in MetaER_pictobert equips the model with the ability to rapidly adapt to new user preferences while retaining past knowledge.\n"," 1. Personalization: It can quickly personalize pictogram recommendations based on minimal user interactions.\n"," \n"," 2. Generalization: It retains a generalized understanding of various user preferences, making it robust across diverse user groups.\n","\n","A major challenge with Continual learning is to mitigate `Catastrophic forgetting` which occurs when a model trained sequentially on multiple tasks forgets previous tasks as it learns new ones. MetaER_pictobert addresses this challenge in several ways:\n","\n"," 1. Experience Replay (ER):\n","    - <i>Memory Buffer for Old Examples</i>: The model maintains a memory buffer that stores examples from past tasks (or user preferences) and regularly replays them during training.\n","    - <i>Periodic Sampling from Memory</i>: During each training step, the model randomly samples from the memory buffer and combines these samples with new data. This ensures that the model retains knowledge of past tasks while learning from new data.\n","    - <i>How This Prevents Forgetting</i>: By continually revisiting old examples, the model prevents the parameters from drifting too far from what was optimal for past tasks, mitigating catastrophic forgetting.\n","\n"," 2. MAML-Inspired Meta-Learning:\n","    - <i>Learned Initialization</i>: The MAML approach learns an initialization of parameters that are effective for rapid adaptation to a wide range of tasks. It trains the model’s parameters such that a small number of gradient updates will lead to fast learning on a new task.\n","    - <i>Inner Loop Updates (Task-Specific Fine-Tuning)</i>: When learning new tasks, the model only fine-tunes a small number of gradient steps, which prevents excessive overwriting of the base parameters.\n","    - <i>Outer Loop Updates (Meta-Optimization)</i>: Meta-optimization ensures that the base parameters remain robust and general, preserving knowledge that is broadly useful across tasks.\n","\n"," 3. Balanced Training Strategy:\n","\n","    - <i>Combining Mini-Batches</i>: The model combines current mini-batches with samples drawn from the memory buffer, ensuring that it is trained on both new and historical data. This reduces the risk of overfitting to recent data or losing valuable information learned earlier.\n","    - <i>Task-Specific Learning (Inner Update)</i>: The task-specific learning process ensures that each new task is adapted with minimal changes to the core parameters, which are meta-optimized to be adaptable yet stable.\n","\n"," 4. Custom Weight Application and Dynamic Updates:\n","\n","    - <i>Dynamic Weight Application</i>: By dynamically applying weights during the meta-learning loop, the model achieves finer control over which parameters are updated, further reducing the risk of forgetting.\n","    - <i>Efficient Memory Management</i>: The cyclic replacement strategy in the memory buffer allows the model to maintain a diverse set of training examples over time, which is crucial for preventing forgetting in a continual learning scenario."]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5552936,"sourceId":9186285,"sourceType":"datasetVersion"}],"dockerImageVersionId":30762,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
